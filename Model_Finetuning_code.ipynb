import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
import torch

class EnhancedSQLDataset(torch.utils.data.Dataset):
    def __init__(self, tokenizer, explanations, queries, codes, max_length=512):
        self.tokenizer = tokenizer
        self.explanations = explanations
        self.queries = queries
        self.codes = codes
        self.max_length = max_length

    def __len__(self):
        return len(self.explanations)

    def __getitem__(self, idx):
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        text = f"[EXPLANATION] {self.explanations[idx]} [QUERY] {self.queries[idx]} [CODE] {self.codes[idx]}"
        encoded = self.tokenizer.encode_plus(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors="pt"
        )

        return {
            'input_ids': encoded['input_ids'].squeeze(),
            'attention_mask': encoded['attention_mask'].squeeze(),
            'labels': encoded['input_ids'].squeeze(),
        }

# Load the tokenizer and model
tokenizer =AutoTokenizer.from_pretrained("Salesforce/codegen-350M-mono")
model = AutoModelForCausalLM.from_pretrained("Salesforce/codegen-350M-mono")

# Mount Google Drive to access files
from google.colab import drive
drive.mount('/content/drive')

# Load your dataset
df = pd.read_csv("/content/drive/MyDrive/SQL_Dataset.csv")  # Adjust the path as per your file location

# Prepare the dataset for training
dataset = EnhancedSQLDataset(
    tokenizer,
    df['explanation'].tolist(),
    df['query'].tolist(),
    df['code'].tolist()
)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,  # Adjust based on your GPU memory
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    save_total_limit=2,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

# Start training
trainer.train()

# Save the model and tokenizer
model.save_pretrained("/content/drive/MyDrive/fine-tuned-codegenmodel-with-explanation")
tokenizer.save_pretrained("/content/drive/MyDrive/fine-tuned-codegenmodel-with-explanation")